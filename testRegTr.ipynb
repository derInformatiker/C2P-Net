{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os, argparse\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from easydict import EasyDict as edict\n",
    "import yaml\n",
    "from matplotlib.pyplot import cm as colormap\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "from regtr.cvhelpers.misc import prepare_logger\n",
    "\n",
    "from regtr.data_loaders import get_dataloader\n",
    "from regtr.models import get_model\n",
    "from regtr.trainer import Trainer\n",
    "from regtr.utils.misc import load_config\n",
    "from regtr.utils.se3_numpy import se3_transform\n",
    "from regtr.utils.se3_torch import se3_transform as se3_transform_torch\n",
    "from regtr.data_loaders.eardataset import EarDataset, EarDatasetTest\n",
    "\n",
    "import regtr.cvhelpers.visualization as cvv\n",
    "import regtr.cvhelpers.colors as colors\n",
    "from regtr.cvhelpers.torch_helpers import to_numpy\n",
    "\n",
    "from deformationpyramid.model.geometry import *\n",
    "from deformationpyramid.model.loss import compute_truncated_chamfer_distance\n",
    "from deformationpyramid.model.registration import Registration\n",
    "from deformationpyramid.utils.benchmark_utils import setup_seed\n",
    "from deformationpyramid.utils.tiktok import Timers\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result_regtr(src_xyz: np.ndarray, tgt_xyz: np.ndarray,\n",
    "                     src_kp: np.ndarray, src2tgt: np.ndarray,\n",
    "                     src_overlap: np.ndarray,\n",
    "                     pose: np.ndarray,\n",
    "                     threshold: float = 0.5):\n",
    "    \"\"\"Visualizes the registration result:\n",
    "       - Top-left: Source point cloud and keypoints\n",
    "       - Top-right: Target point cloud and predicted corresponding kp positions\n",
    "       - Bottom-left: Source and target point clouds before registration\n",
    "       - Bottom-right: Source and target point clouds after registration\n",
    "    Press 'q' to exit.\n",
    "    Args:\n",
    "        src_xyz: Source point cloud (M x 3)\n",
    "        tgt_xyz: Target point cloud (N x 3)\n",
    "        src_kp: Source keypoints (M' x 3)\n",
    "        src2tgt: Corresponding positions of src_kp in target (M' x 3)\n",
    "        src_overlap: Predicted probability the point lies in the overlapping region\n",
    "        pose: Estimated rigid transform\n",
    "        threshold: For clarity, we only show predicted overlapping points (prob > 0.5).\n",
    "                   Set to 0 to show all keypoints, and a larger number to show\n",
    "                   only points strictly within the overlap region.\n",
    "    \"\"\"\n",
    "\n",
    "    small_pt_size = 4\n",
    "    large_pt_size = 8\n",
    "    color_mapper = colormap.ScalarMappable(norm=None, cmap=colormap.get_cmap('coolwarm'))\n",
    "    overlap_colors = (color_mapper.to_rgba(src_overlap[:, 0])[:, :3] * 255).astype(np.uint8)\n",
    "    m = src_overlap[:, 0] > threshold\n",
    "\n",
    "    vis = cvv.Visualizer(\n",
    "        win_size=(1600, 1000),\n",
    "        num_renderers=4,\n",
    "        bg_color=(255, 255, 255)\n",
    "    )\n",
    "    \n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(src_xyz, colors=colors.BLUE, pt_size=small_pt_size, alpha=0.25),\n",
    "        renderer_idx=0\n",
    "    )\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(src_kp[m, :], colors=overlap_colors[m, :], pt_size=large_pt_size),\n",
    "        renderer_idx=0\n",
    "    )\n",
    "\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(tgt_xyz, colors=colors.ORANGE, pt_size=small_pt_size, alpha=0.25),\n",
    "        renderer_idx=1\n",
    "    )\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(src2tgt[m, :], colors=overlap_colors[m, :], pt_size=large_pt_size),\n",
    "        renderer_idx=1\n",
    "    )\n",
    "\n",
    "    # Before registration\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(src_xyz, colors=colors.BLUE, pt_size=small_pt_size),\n",
    "        renderer_idx=2\n",
    "    )\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(tgt_xyz, colors=colors.ORANGE, pt_size=small_pt_size),\n",
    "        renderer_idx=2\n",
    "    )\n",
    "\n",
    "    # After registration\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(se3_transform(pose, src_xyz), colors=colors.BLUE, pt_size=small_pt_size),\n",
    "        renderer_idx=3\n",
    "    )\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(tgt_xyz, colors=colors.ORANGE, pt_size=small_pt_size),\n",
    "        renderer_idx=3\n",
    "    )\n",
    "\n",
    "    vis.set_titles(['Source point cloud (with keypoints)',\n",
    "                    'Target point cloud (with predicted source keypoint positions)',\n",
    "                    'Before Registration',\n",
    "                    'After Registration'])\n",
    "\n",
    "    vis.reset_camera()\n",
    "    vis.start()\n",
    "\n",
    "def visualize_result(src_xyz: np.ndarray, tgt_xyz: np.ndarray,\n",
    "                     displacement: np.ndarray, inds: np.ndarray):\n",
    "    \"\"\"Visualizes the registration result:\n",
    "       - Top-left: Source point cloud and keypoints\n",
    "       - Top-right: Target point cloud and predicted corresponding kp positions\n",
    "       - Bottom-left: Source and target point clouds before registration\n",
    "       - Bottom-right: Source and target point clouds after registration\n",
    "    Press 'q' to exit.\n",
    "    Args:\n",
    "        src_xyz: Source point cloud (M x 3)\n",
    "        tgt_xyz: Target point cloud (N x 3)\n",
    "        displacement: vector field to transform src to tgt non-rigidly\n",
    "    \"\"\"\n",
    "\n",
    "    color_mapper = colormap.ScalarMappable(norm=None, cmap=colormap.get_cmap('coolwarm'))\n",
    "\n",
    "    vis = cvv.Visualizer(\n",
    "        win_size=(1600, 1000),\n",
    "        num_renderers=2)\n",
    "\n",
    "    # Before registration\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(src_xyz, colors=colors.BLUE),\n",
    "        renderer_idx=0\n",
    "    )\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(tgt_xyz, colors=colors.ORANGE),\n",
    "        renderer_idx=0\n",
    "    )\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(src_xyz[inds], colors=colors.BLUE, pt_size=4),\n",
    "        renderer_idx=0\n",
    "    )\n",
    "\n",
    "    # After registration\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(src_xyz + displacement, colors=colors.BLUE),\n",
    "        renderer_idx=1\n",
    "    )\n",
    "    vis.add_object(\n",
    "        cvv.create_point_cloud(tgt_xyz, colors=colors.ORANGE),\n",
    "        renderer_idx=1\n",
    "    )\n",
    "\n",
    "    vis.set_titles(['Before Registration', 'After Registration'])\n",
    "\n",
    "    vis.reset_camera()\n",
    "    vis.start()\n",
    "\n",
    "with open('mesh_dataset/landmarks/landmarks.pkl', 'rb') as f:\n",
    "    landmarks = pickle.load(f)\n",
    "l_inds = [torch.tensor(v) for u, v in landmarks.items()]\n",
    "\n",
    "def mean_displacement_error(dis_pred, dis_gt):\n",
    "    return np.linalg.norm(dis_pred.cpu()-dis_gt.cpu(), axis=1).mean()\n",
    "\n",
    "def denorm(arr, metadata):\n",
    "    return arr * metadata['std'] + metadata['mean']\n",
    "\n",
    "def landmark_loss(pred, intra):\n",
    "    assert len(pred) == len(intra), 'len(pred) != len(intra)'\n",
    "    l = []\n",
    "    single_loss = {}\n",
    "    segments = list(intra.keys())\n",
    "    for seg in range(len(pred)):\n",
    "        mat = cdist(pred[seg], intra[segments[seg]]).min(0)\n",
    "        \n",
    "        l.append(mat.mean())\n",
    "        single_loss[segments[seg]] = mat.mean()\n",
    "    return sum(l)/len(l), single_loss\n",
    "\n",
    "def computeCDRegTr(src_xyz: torch.tensor, tgt_xyz: torch.tensor, pose: torch.tensor, metadata: dict, red='mean'):\n",
    "    transformed = se3_transform_torch(pose, src_xyz)\n",
    "    transformed = denorm(transformed, metadata)\n",
    "    return compute_truncated_chamfer_distance(transformed, denorm(tgt_xyz, metadata), batch_reduction=red, trunc=1e6)\n",
    "\n",
    "def computeCD(src_xyz: torch.tensor, tgt_xyz: torch.tensor, displ: torch.tensor, red='mean'):\n",
    "    transformed = displ + src_xyz\n",
    "    return compute_truncated_chamfer_distance(transformed, tgt_xyz, batch_reduction=red, trunc=1e6).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conf:\n",
    "    pass\n",
    "conf.benchmark = '3DMatch'\n",
    "conf.config = 'config/eardataset_regtr.yaml'\n",
    "conf.logdir = 'logs'\n",
    "conf.dev = False\n",
    "conf.num_workers = 0\n",
    "conf.name = None\n",
    "#conf.resume = 'D:/logs/eardataset/230224_074601_regtr_eardataset_standard/ckpt/model-88000.pth'\n",
    "conf.resume = 'D:/mesh2mesh/trainResults/eardataset/240107_080636_regtr_eardataset_standard_resume/ckpt/model-66000.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m02/27 22:00:33\u001b[0m \u001b[1;30m[INFO]\u001b[0m \u001b[34mroot\u001b[0m - Output and logs will be saved to logs\\240227_220033\n",
      "\u001b[32m02/27 22:00:33\u001b[0m \u001b[1;30m[INFO]\u001b[0m \u001b[34mregtr.cvhelpers.misc\u001b[0m - Command: C:\\Users\\chenp\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py --f=c:\\Users\\chenp\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-21752Hx2tmn2EHxlU.json\n",
      "\u001b[32m02/27 22:00:33\u001b[0m \u001b[1;30m[INFO]\u001b[0m \u001b[34mregtr.cvhelpers.misc\u001b[0m - Source is from Commit 95412c13 (2023-09-06): Fixed one small BUG in test_script.py!\n",
      "\u001b[32m02/27 22:00:33\u001b[0m \u001b[1;30m[INFO]\u001b[0m \u001b[34mregtr.cvhelpers.misc\u001b[0m - Arguments: \n",
      "\u001b[32m02/27 22:00:34\u001b[0m \u001b[1;30m[INFO]\u001b[0m \u001b[34mRegTR\u001b[0m - Instantiating model RegTR\n",
      "\u001b[32m02/27 22:00:35\u001b[0m \u001b[1;30m[INFO]\u001b[0m \u001b[34mRegTR\u001b[0m - Loss weighting: {'overlap_5': 1.0, 'feature_5': 0.1, 'corr_5': 1.0, 'feature_un': 0.0}\n",
      "\u001b[32m02/27 22:00:35\u001b[0m \u001b[1;30m[INFO]\u001b[0m \u001b[34mRegTR\u001b[0m - Config: d_embed:64, nheads:8, pre_norm:True, use_pos_emb:True, sa_val_has_pos_emb:True, ca_val_has_pos_emb:True\n"
     ]
    }
   ],
   "source": [
    "opt = conf()\n",
    "logger, opt.log_path = prepare_logger(opt)\n",
    "# Override config if --resume is passed\n",
    "if opt.config is None:\n",
    "    if opt.resume is None or not os.path.exists(opt.resume):\n",
    "        logger.error('--config needs to be supplied unless resuming from checkpoint')\n",
    "        exit(-1)\n",
    "    else:\n",
    "        resume_folder = opt.resume if os.path.isdir(opt.resume) else os.path.dirname(opt.resume)\n",
    "        opt.config = os.path.normpath(os.path.join(resume_folder, '../config.yaml'))\n",
    "        if os.path.exists(opt.config):\n",
    "            logger.info(f'Using config file from checkpoint directory: {opt.config}')\n",
    "        else:\n",
    "            logger.error('Config not found in resume directory')\n",
    "            exit(-2)\n",
    "else:\n",
    "    # Save config to log\n",
    "    config_out_fname = os.path.join(opt.log_path, 'config.yaml')\n",
    "    with open(opt.config, 'r') as in_fid, open(config_out_fname, 'w') as out_fid:\n",
    "        out_fid.write(f'# Original file name: {opt.config}\\n')\n",
    "        out_fid.write(in_fid.read())\n",
    "\n",
    "cfg = EasyDict(load_config(opt.config))\n",
    "with open(os.path.join(cfg.root,'metadata.pkl'), 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "with open(cfg.ndp_config_path,'r') as f:\n",
    "    p_config = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "p_config = edict(p_config)\n",
    "p_config.device = torch.cuda.current_device()\n",
    "\n",
    "cfg.dataset = 'eardataset_test'\n",
    "test_loader = get_dataloader(cfg, phase='test')\n",
    "Model = get_model(cfg.model)\n",
    "model = Model(cfg)\n",
    "trainer = Trainer(opt, niter=cfg.niter, grad_clip=cfg.grad_clip)\n",
    "\n",
    "model_nonrigid = Registration(p_config)\n",
    "timer = Timers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m02/27 22:00:39\u001b[0m \u001b[1;30m[INFO]\u001b[0m \u001b[34mCheckPointManager\u001b[0m - Loaded models from D:/mesh2mesh/trainResults/eardataset/240107_080636_regtr_eardataset_standard_resume/ckpt/model-66000.pth\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outputs = trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_src = []\n",
    "pred_tgt = []\n",
    "tgt_gt = []\n",
    "pred_pose = []\n",
    "pred_src_kp = []\n",
    "pred_tgt_kp = []\n",
    "pred_overlap_score = []\n",
    "displ_gt = []\n",
    "inds = []\n",
    "src_paths = []\n",
    "for sample in outputs:\n",
    "    pred_src.extend(sample['src_xyz'])\n",
    "    pred_tgt.extend(sample['tgt_xyz'])\n",
    "    tgt_gt.extend(sample['full_tgt_xyz'])\n",
    "    pred_pose.extend(sample['pose'][-1])\n",
    "    pred_src_kp.extend(sample['src_kp'])\n",
    "    pred_tgt_kp.extend(sample['src_kp_warped'])\n",
    "    pred_overlap_score.extend(sample['src_overlap'])\n",
    "    displ_gt.extend(sample['displ_gt'])\n",
    "    src_paths.extend(sample['src_path'])\n",
    "\n",
    "pred_src = torch.stack(pred_src)\n",
    "tgt_gt = torch.stack(tgt_gt)\n",
    "pred_pose = torch.stack(pred_pose)\n",
    "pred_src_kp = torch.stack(pred_src_kp)\n",
    "pred_tgt_kp = torch.stack(pred_tgt_kp)[:, -1]\n",
    "pred_overlap_score = torch.sigmoid(torch.stack(pred_overlap_score)[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [05:26,  7.60s/it]\n"
     ]
    }
   ],
   "source": [
    "nn_thresh = 0.5\n",
    "\n",
    "transformations = []\n",
    "nonregistered_cd_l, registered_cd_l, mean_displacement_error_l, landmark_loss_l = [], [], [], []\n",
    "anulus, umbo, malleus_handle, long_process_of_incus, stape = [], [], [], [], []\n",
    "regtr_cd_l = []\n",
    "non_registered_landmark_loss_l = []\n",
    "registered_partial_cd_l = []\n",
    "wall_time = []\n",
    "displ_l = []\n",
    "side, status = [], []\n",
    "cnt = 0\n",
    "\n",
    "test_dataset_real = EarDatasetTest(\n",
    "    cfg, 'test'\n",
    ")\n",
    "\n",
    "for pair_ind, (src_norm, tgt_norm, tgt_full_norm, src_kp_norm, overlap_score, pose, displ, src_path) in tqdm(enumerate(zip(pred_src, pred_tgt, tgt_gt, pred_src_kp, pred_overlap_score, pred_pose, displ_gt, src_paths))):\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Use KDTree to compute correnspondences\n",
    "    tree = KDTree(src_norm.cpu(), leaf_size=cfg.nn_leaf_size)\n",
    "    distance, indices = tree.query((src_kp_norm[overlap_score.squeeze() > nn_thresh]).cpu(), k=10, return_distance=True)\n",
    "    indices = np.concatenate(indices)\n",
    "    distance = np.concatenate(distance)\n",
    "    indices, occ = np.unique(indices[distance < 0.1], return_counts=True)\n",
    "    inds.append(indices)\n",
    "\n",
    "    # Transform and denorm pcds\n",
    "    src_transformed_norm = torch.tensor(se3_transform(pose.cpu().numpy(), src_norm.cpu().numpy()))\n",
    "    src_denorm = denorm(src_norm, metadata)\n",
    "    src_transformed_denorm = denorm(src_transformed_norm, metadata)\n",
    "    tgt_denorm = denorm(tgt_norm, metadata)\n",
    "    tgt_full_denorm = denorm(tgt_full_norm, metadata)\n",
    "\n",
    "    # NDP\n",
    "    model_nonrigid.load_pcds(src_transformed_denorm, tgt_denorm, inds=indices, search_radius=0.0375)\n",
    "    warped_pcd, hist, _, timer = model_nonrigid.register(visualize=False, timer = timer)\n",
    "    pred_displ = warped_pcd - src_denorm\n",
    "    displ_l.append(pred_displ)\n",
    "    wall_time.append(time.time()-t1)\n",
    "    #registered_partial_cd_l.append(computeCD(src_denorm[indices][None], tgt_full_denorm[None], pred_displ[indices][None]).item())\n",
    "    \n",
    "    # Compute Metrics\n",
    "    if cfg.dataset == 'eardataset':\n",
    "        nonregistered_cd_l.append(compute_truncated_chamfer_distance(src_denorm[None], tgt_full_denorm[None], trunc=1e6).item())\n",
    "\n",
    "        registered_cd_l.append(computeCD(src_denorm[None], tgt_full_denorm[None], pred_displ[None]).item())\n",
    "\n",
    "        mean_displacement_error_l.append(mean_displacement_error(pred_displ, displ).item())\n",
    "\n",
    "        pred_landmarks = [warped_pcd[i].cpu() for i in l_inds]\n",
    "        pre_landmarks = [tgt_full_denorm[i].cpu() for i in l_inds]\n",
    "        lndmk = landmark_loss(pre_landmarks, pred_landmarks)\n",
    "        landmark_loss_l.append(lndmk)\n",
    "        regtr_cd_l.append(computeCD(src_transformed_denorm.cpu()[None], tgt_full_denorm.cpu()[None], torch.zeros(src_transformed_denorm.shape)[None]).item())\n",
    "\n",
    "    else:\n",
    "        sample_name = src_path.split('\\\\')[-1].split('.')[0]\n",
    "\n",
    "        nonregistered_cd_l.append(compute_truncated_chamfer_distance(src_denorm[None], tgt_denorm[None], trunc=1e6).item())\n",
    "\n",
    "        registered_cd_l.append(computeCD(src_denorm[None], tgt_denorm[None], pred_displ[None]).item())\n",
    "\n",
    "        intra_data = test_dataset_real.__getitem__(pair_ind)\n",
    "        intra_metadata = intra_data['metadata']\n",
    "        side.append(intra_metadata['patient_info']['side'])\n",
    "        status.append(intra_metadata['patient_info']['status'])\n",
    "        \n",
    "        landmarks_intra = intra_data['landmarks']\n",
    "        \n",
    "        if landmarks_intra != {}:\n",
    "            pred_landmarks = [warped_pcd[landmarks[k]].cpu() for k, v in landmarks_intra.items()]\n",
    "            intra_landmarks = {k:v for k, v in landmarks_intra.items()}\n",
    "            lndmk, single_loss = landmark_loss(pred_landmarks, intra_landmarks)\n",
    "            landmark_loss_l.append(lndmk)\n",
    "\n",
    "            segments = list(single_loss.keys())\n",
    "            if 'anulus' in segments:\n",
    "                anulus.append(single_loss['anulus'])\n",
    "            else:\n",
    "                anulus.append(float('nan'))\n",
    "\n",
    "            if 'Umbo' in segments:\n",
    "                umbo.append(single_loss['Umbo'])\n",
    "            else:\n",
    "                umbo.append(float('nan'))\n",
    "            \n",
    "            if 'malleus handle' in segments:\n",
    "                malleus_handle.append(single_loss['malleus handle'])\n",
    "            else:\n",
    "                malleus_handle.append(float('nan'))\n",
    "\n",
    "            if 'long process of incus' in segments:\n",
    "                long_process_of_incus.append(single_loss['long process of incus'])\n",
    "            else:\n",
    "                long_process_of_incus.append(float('nan'))\n",
    "\n",
    "            if 'stape' in segments:\n",
    "                stape.append(single_loss['stape'])\n",
    "            else:\n",
    "                stape.append(float('nan'))\n",
    "\n",
    "            pred_landmarks = [src_denorm[landmarks[k]].cpu() for k, v in landmarks_intra.items()]\n",
    "            lndmk, _ = landmark_loss(pred_landmarks, intra_landmarks)\n",
    "            non_registered_landmark_loss_l.append(lndmk)\n",
    "\n",
    "        else:\n",
    "            landmark_loss_l.append(-1)\n",
    "            anulus.append(float('nan'))\n",
    "            umbo.append(float('nan'))\n",
    "            malleus_handle.append(float('nan'))\n",
    "            long_process_of_incus.append(float('nan'))\n",
    "            stape.append(float('nan'))\n",
    "            non_registered_landmark_loss_l.append(-1)\n",
    "    oct_pcd = o3d.geometry.PointCloud()\n",
    "    oct_pcd.points = o3d.utility.Vector3dVector(np.array(warped_pcd.cpu().detach()))\n",
    "    o3d.io.write_point_cloud(f'test_output_folder/predictions/prediction_{pair_ind}.ply', oct_pcd)\n",
    "    oct_pcd = o3d.geometry.PointCloud()\n",
    "    oct_pcd.points = o3d.utility.Vector3dVector(np.array(tgt_denorm.cpu().detach()))\n",
    "    o3d.io.write_point_cloud(f'test_output_folder/target shape/target_{pair_ind}.ply', oct_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-registered cd score: 7.3261259766512135\n",
      "Registered cd score: 0.8497772393531577\n",
      "Landmark loss: 1.3019973623443006\n",
      "Wall time: 7.563937364622604 s\n"
     ]
    }
   ],
   "source": [
    "print('Non-registered cd score:', mean(nonregistered_cd_l))\n",
    "print('Registered cd score:', mean(registered_cd_l))\n",
    "#print('Registered partial cd score:', mean(registered_partial_cd_l))\n",
    "if cfg.dataset == 'eardataset':\n",
    "    print('Mean displacement error: ', mean(mean_displacement_error_l))\n",
    "print('Landmark loss:', mean([i for i in landmark_loss_l if i != -1]))\n",
    "print('Wall time:', mean(wall_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_result(src_denorm.cuda(),  tgt_denorm, pred_displ.cuda(), indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 4\n",
    "visualize_result_regtr(pred_src[ind].cpu().numpy(), pred_tgt[ind].cpu().numpy(), pred_src_kp[ind].cpu().numpy(), pred_tgt_kp[ind].cpu().numpy(), pred_overlap_score[ind].cpu().numpy(), pred_pose[ind].cpu().numpy(), threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(43):\n",
    "    data = dict(\n",
    "        src = denorm(se3_transform(pred_pose[ind].cpu().numpy(), pred_src[ind].cpu().numpy()), metadata),\n",
    "        inds = inds[i],\n",
    "    )\n",
    "    with open(f'mesh_dataset/DIOME_FanShapeCorr/sample_{i}/regtr_pred.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = KDTree(src_norm.cpu(), leaf_size=cfg.nn_leaf_size)\n",
    "indices = tree.query_radius((src_kp_norm[overlap_score.squeeze() > nn_thresh]).cpu(), r=0.1)\n",
    "indices, occurences = np.unique(np.concatenate(indices), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" On normal dataset\n",
    "Non-registered cd score: 1.4596144970549785\n",
    "Registered cd score: 0.29597098740516437\n",
    "Registered partial cd score: 2.3302804929076717\n",
    "Mean displacement error:  1.2692070097504926\n",
    "Landmark loss: 0.3599099684169502\n",
    "Wall time: 8.540204111336594 s \"\"\"\n",
    "\n",
    "\"\"\" On high variety dataset\n",
    "Non-registered cd score: 1.4065552084325432\n",
    "Registered cd score: 1.0310056967748418\n",
    "Registered partial cd score: 1.5106343214397315\n",
    "Mean displacement error:  1.7764664716548748\n",
    "Landmark loss: 0.8699200562834195\n",
    "Wall time: 6.008897996879555 s \n",
    "\n",
    "On corrected DIOME\n",
    "Non-registered cd score: 4.179325458615325\n",
    "Registered cd score: 0.8000558288984521\n",
    "Landmark loss: 1.2649516742809437\n",
    "Wall time: 6.067085615424222 s\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_invivo = dict(\n",
    "    nonregistered_cd_l=nonregistered_cd_l,\n",
    "    non_registered_landmark_loss_l=non_registered_landmark_loss_l,\n",
    "    registered_cd_l=registered_cd_l,\n",
    "    landmark_loss_l=[i if i != -1 else float('nan') for i in landmark_loss_l],\n",
    "    wall_time_models=wall_time,\n",
    "    displ=[i.cpu().numpy() for i in displ_l],\n",
    "    #inds=inds,\n",
    "    side=side,\n",
    "    status=status,\n",
    "    anulus=anulus,\n",
    "    umbo=umbo,\n",
    "    malleus_handle=malleus_handle,\n",
    "    long_process_of_incus=long_process_of_incus,\n",
    "    stape=stape\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results_dict_invivo)\n",
    "df.to_csv('finalResults/results_invivo_regtr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict_exvivo = dict(\n",
    "    nonregistered_cd_l=nonregistered_cd_l,\n",
    "    #non_registered_landmark_loss_l=non_registered_landmark_loss_l,\n",
    "    mean_displacement_error_l=mean_displacement_error_l,\n",
    "    registered_cd_l=registered_cd_l,\n",
    "    regtr_cd_l=regtr_cd_l,\n",
    "    landmark_loss_l=[i if i != -1 else float('nan') for i in landmark_loss_l],\n",
    "    wall_time_models=wall_time,\n",
    "    displ=[i.cpu().numpy() for i in displ_l],\n",
    "    len_inds=[len(i) for i in inds],\n",
    "    overlap_scores=number_of_points\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(results_dict_exvivo)\n",
    "df.to_csv('finalResults/results_exvivo_regtr_variance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh as trm\n",
    "from glob import glob\n",
    "\n",
    "values = []\n",
    "\n",
    "for path in glob('mesh_dataset/DIOME_FanShapeCorr/sample_*'):\n",
    "    summm = 0\n",
    "    parts = dict(\n",
    "        tympanic_membrane = 0,\n",
    "        malleus = 0,\n",
    "        incus = 0,\n",
    "        stapes = 0,\n",
    "    )\n",
    "    \n",
    "    for mesh_path in glob(f'{path}/seg_*.stl'):\n",
    "        if \"tympanic_membrane\" in mesh_path:\n",
    "            index = 'tympanic_membrane'\n",
    "        elif \"malleus\" in mesh_path:\n",
    "            index = 'malleus'\n",
    "        elif \"incus\" in mesh_path:\n",
    "            index = 'incus'\n",
    "        if \"stapes\" in mesh_path:\n",
    "            index = 'stapes'\n",
    "        elif \"promontory\" in mesh_path:\n",
    "            index = 4\n",
    "            continue\n",
    "        s = trm.load(mesh_path).volume\n",
    "        parts[index] = s\n",
    "        summm += s\n",
    "    parts = {k: v/summm for k, v in parts.items()}\n",
    "    print(sum([v for k, v in parts.items()]))\n",
    "    if sum([v for k, v in parts.items()]) > 0.9:\n",
    "        values.append(parts)\n",
    "\n",
    "with open('mesh_dataset/DIOME_FanShapeCorr/volume_distribution.pkl', 'wb') as f:\n",
    "    pickle.dump(values, f)\n",
    "\n",
    "values = []\n",
    "\n",
    "for filename in glob('mesh_dataset/ear_dataset/*/data_cached.pkl'):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    segmentation = list(data['intra_segmentation'])\n",
    "    sum = len(segmentation)\n",
    "    values.append(dict(\n",
    "        tympanic_membrane = segmentation.count(2)/sum,\n",
    "        malleus = segmentation.count(1)/sum,\n",
    "        incus = segmentation.count(0)/sum,\n",
    "        stapes = segmentation.count(3)/sum,\n",
    "    ))\n",
    "\n",
    "with open('mesh_dataset/ear_dataset/volume_distribution.pkl', 'wb') as f:\n",
    "    pickle.dump(values, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36fc8a2fd897a0d1e2bc574a8cb753f8bb10edc5df5276cb1cc692295409816c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
