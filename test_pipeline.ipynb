{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS FOR NGENET AND DEFORMATION PYRAMID\n",
    "import os\n",
    "from glob import glob\n",
    "from pickle import load, dump\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import trimesh as trm\n",
    "import yaml\n",
    "import time\n",
    "from scipy.spatial.distance import cdist\n",
    "from statistics import mean\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from deformationpyramid.model.geometry import *\n",
    "from deformationpyramid.model.loss import compute_truncated_chamfer_distance\n",
    "from deformationpyramid.model.registration import Registration\n",
    "from tqdm import tqdm\n",
    "from ngenet.data import EarDataset, EarDatasetTest, get_dataloader\n",
    "from ngenet.models import NgeNet, architectures, vote\n",
    "from ngenet.utils import (decode_config, execute_global_registration, get_blue,\n",
    "                          get_correspondences, get_yellow, npy2feat, npy2pcd, pcd2npy,\n",
    "                          setup_seed, to_tensor, vis_plys)\n",
    "from deformationpyramid.utils.benchmark_utils import setup_seed\n",
    "from deformationpyramid.utils.tiktok import Timers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(loader, node):\n",
    "    seq = loader.construct_sequence(node)\n",
    "    return '_'.join([str(i) for i in seq])\n",
    "yaml.add_constructor('!join', join)\n",
    "\n",
    "def checkIPython():\n",
    "    try:\n",
    "        get_ipython().__class__.__name__\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if checkIPython(): # Checks if running in IPython notebook. If running by CLI, argparse is used\n",
    "    class config:\n",
    "        pass\n",
    "    args = config()\n",
    "    args.data_root = 'mesh_dataset/ear_dataset/'\n",
    "    args.dataset_split = 'test'\n",
    "    args.oct_data_root = 'mesh_dataset/oct_outputs/*.npy'\n",
    "    args.checkpoint = 'trainResults/eardataset_nonrigid_randrot_pretrained_eardrum_large_ds/checkpoints/best_recall.pth'\n",
    "    #args.checkpoint = 'trainResults/trained_03_13_100k_randomsplit_0/checkpoints/best_recall.pth'\n",
    "    args.vis = False\n",
    "    args.no_cuda = False\n",
    "    args.use_real = True\n",
    "\n",
    "    args.ngenet_config_path = 'config/eardataset_ngenet.yaml'\n",
    "    args.ndp_config_path = 'config/NDP.yaml'\n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--data_root', type=str, default='mesh_dataset/ear_dataset/', help='root of synthetic dataset')\n",
    "    parser.add_argument('--dataset_split', type=str, default='val', help='which of the splits should be used as synthetic dataset')\n",
    "    parser.add_argument('--oct_data_root', type=str, default='mesh_dataset/oct_outputs/*.npy', help='glob specification of all oct scans to test (convert to .npy first)')\n",
    "    parser.add_argument('--checkpoint', type=str, default='trainResults/eardataset_nonrigid_randrot_pretrained_eardrum_large_ds/checkpoints/best_loss.pth', help='path to NgeNet checkpoint')\n",
    "    parser.add_argument('--ngenet_config_path', type=str, default='config/eardataset.yaml', help='which configuration file to use for NgeNet')\n",
    "    parser.add_argument('--ndp_config_path', type=str, default='config/NDP.yaml', help='which configuration file to use for NDP')\n",
    "    parser.add_argument('--use_real', action='store_true', default=False, help='decide wheather to run the test on the oct scans')\n",
    "    parser.add_argument('--vis', action='store_true',  default=False, help='visualize output while running in an extra window')\n",
    "    parser.add_argument('--no_cuda', action='store_true',  default=False, help='disable cuda (cpu only)')\n",
    "\n",
    "    args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(22)\n",
    "config = decode_config(args.ngenet_config_path)\n",
    "config = edict(config)\n",
    "config.architecture = architectures[config.dataset]\n",
    "config.num_workers = 0\n",
    "\n",
    "diome_path = 'mesh_dataset/DIOME_FanShapeCorr/'\n",
    "\n",
    "test_dataset = EarDataset(\n",
    "    root=args.data_root,\n",
    "    noisy_intra=config.noisy_intra,\n",
    "    split=args.dataset_split,\n",
    "    aug=False,\n",
    "    overlap_radius=config.overlap_radius\n",
    ")\n",
    "\n",
    "test_dataset_real = EarDatasetTest(\n",
    "    test_path=diome_path,\n",
    "    root=args.data_root,\n",
    "    noisy_intra=config.noisy_intra,\n",
    "    split='test',\n",
    "    aug=False,\n",
    "    overlap_radius=config.overlap_radius\n",
    ")\n",
    "\n",
    "metadata = test_dataset.metadata\n",
    "\n",
    "test_dataloader, neighborhood_limits = get_dataloader(\n",
    "    config=config,\n",
    "    dataset=test_dataset_real if args.use_real else test_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    num_workers=config.num_workers,\n",
    "    shuffle=False,\n",
    "    neighborhood_limits=None\n",
    ")\n",
    "\n",
    "with open(args.ndp_config_path,'r') as f:\n",
    "    p_config = yaml.load(f, Loader=yaml.Loader)\n",
    "\n",
    "p_config = edict(p_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mesh_dataset/landmarks/landmarks.pkl', 'rb') as f:\n",
    "    landmarks = load(f)\n",
    "\n",
    "def registration_cd(points_pred, points_tgt, corrs, pred_T=np.eye(4)):\n",
    "    '''\n",
    "\n",
    "    :param points_pred: (n, 3)\n",
    "    :param points_tgt: (m, 3)\n",
    "    :return: float\n",
    "    '''\n",
    "    if len(corrs) != 0:\n",
    "        R, t = pred_T[:3, :3], pred_T[:3, 3]\n",
    "        points_pred = points_pred @ R.T + t\n",
    "        points_pred = torch.tensor(points_pred[corrs]).T.unsqueeze(0).float()\n",
    "        points_tgt = torch.tensor(points_tgt).T.unsqueeze(0).float()\n",
    "        cd = compute_truncated_chamfer_distance(points_pred, points_tgt, trunc=1e+9)\n",
    "        cd = cd.item()\n",
    "    else:\n",
    "        cd = 1e+4\n",
    "    return cd\n",
    "\n",
    "def mean_displacement_error(dis_pred, dis_gt):\n",
    "    return np.linalg.norm(dis_pred-dis_gt, axis=1).mean()\n",
    "\n",
    "def denorm(arr, metadata):\n",
    "    return arr * metadata['std'] + metadata['mean']\n",
    "\n",
    "def landmark_loss(pred, intra):\n",
    "    assert len(pred) == len(intra), 'len(pred) != len(intra)'\n",
    "    l = []\n",
    "    single_loss = {}\n",
    "    segments = list(intra.keys())\n",
    "    for seg in range(len(pred)):\n",
    "        mat = cdist(pred[seg], intra[segments[seg]]).min(0)\n",
    "        \n",
    "        l.append(mat.mean())\n",
    "        single_loss[segments[seg]] = mat.mean()\n",
    "    return sum(l)/len(l), single_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rigid = NgeNet(config)\n",
    "use_cuda = not args.no_cuda\n",
    "if use_cuda:\n",
    "    model = model_rigid.cuda()\n",
    "    if args.checkpoint != None:\n",
    "        model_rigid.load_state_dict(torch.load(args.checkpoint))\n",
    "    p_config.device = torch.cuda.current_device()\n",
    "else:\n",
    "    model_rigid.load_state_dict(\n",
    "        torch.load(args.checkpoint, map_location=torch.device('cpu')))\n",
    "    config.device = torch.device('cpu')\n",
    "model_rigid.eval()\n",
    "\n",
    "fmr_threshold = 0.05\n",
    "rmse_threshold = 0.2\n",
    "inlier_ratios, mutual_inlier_ratios = [], []\n",
    "mutual_feature_match_recalls, feature_match_recalls = [], []\n",
    "transformations = []\n",
    "nonregistered_cd_l, registered_cd_l, mean_displacement_error_l, landmark_loss_l, ngenet_cd_l = [], [], [], [], []\n",
    "overlap_scores, wall_time_models = [], []\n",
    "displ_ngenet, displ = [], []\n",
    "mdes, visible_point_ratio = [], []\n",
    "uncertainty_scores = []\n",
    "side, status = [], []\n",
    "len_coors_l = []\n",
    "\n",
    "dist_thresh_maps = {\n",
    "    '10000': config.first_subsampling_dl,\n",
    "    '5000': config.first_subsampling_dl,\n",
    "    '2500': config.first_subsampling_dl * 1.5,\n",
    "    '1000': config.first_subsampling_dl * 1.5,\n",
    "    '500': config.first_subsampling_dl * 1.5,\n",
    "    '250': config.first_subsampling_dl * 2,\n",
    "}\n",
    "\n",
    "model_nonrigid = Registration(p_config)\n",
    "timer = Timers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [06:14<00:00,  8.71s/it]\n"
     ]
    }
   ],
   "source": [
    "anulus, umbo, malleus_handle, long_process_of_incus, stape = [], [], [], [], []\n",
    "\n",
    "for pair_ind, inputs in enumerate(tqdm(test_dataloader)):\n",
    "    t1 = time.time()\n",
    "    if use_cuda:\n",
    "        for k, v in inputs.items():\n",
    "            if isinstance(v, list):\n",
    "                for i in range(len(v)):\n",
    "                    inputs[k][i] = inputs[k][i].cuda()\n",
    "            else:\n",
    "                inputs[k] = inputs[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        batched_feats_h, batched_feats_m, batched_feats_l = model_rigid(inputs)\n",
    "        stack_points = inputs['points']\n",
    "        stack_points_raw = inputs['batched_points_raw']\n",
    "        stack_lengths = inputs['stacked_lengths']\n",
    "        coords_tgt_full = inputs['points_tgt_full']\n",
    "        coords_src = stack_points[0][:stack_lengths[0][0]]\n",
    "        coords_tgt = stack_points[0][stack_lengths[0][0]:]\n",
    "        coords_src_raw = stack_points_raw[:stack_lengths[0][0]]\n",
    "        coords_tgt_raw = stack_points_raw[stack_lengths[0][0]:]\n",
    "        feats_src_h = batched_feats_h[:stack_lengths[0][0]]\n",
    "        feats_tgt_h = batched_feats_h[stack_lengths[0][0]:]\n",
    "        feats_src_m = batched_feats_m[:stack_lengths[0][0]]\n",
    "        feats_tgt_m = batched_feats_m[stack_lengths[0][0]:]\n",
    "        feats_src_l = batched_feats_l[:stack_lengths[0][0]]\n",
    "        feats_tgt_l = batched_feats_l[stack_lengths[0][0]:]\n",
    "\n",
    "        displacement_gt = inputs['transf'][0].detach().cpu().numpy() * metadata['std']\n",
    "\n",
    "        source_npy = coords_src.detach().cpu().numpy()\n",
    "        target_npy = coords_tgt.detach().cpu().numpy()\n",
    "\n",
    "        source_feats_h = feats_src_h[:, :-2].detach().cpu().numpy()\n",
    "        target_feats_h = feats_tgt_h[:, :-2].detach().cpu().numpy()\n",
    "        source_feats_m = feats_src_m.detach().cpu().numpy()\n",
    "        target_feats_m = feats_tgt_m.detach().cpu().numpy()\n",
    "        source_feats_l = feats_src_l.detach().cpu().numpy()\n",
    "        target_feats_l = feats_tgt_l.detach().cpu().numpy() \n",
    "        \n",
    "        after_vote = vote(\n",
    "            source_npy=source_npy, \n",
    "            target_npy=target_npy, \n",
    "            source_feats=[source_feats_h, source_feats_m, source_feats_l], \n",
    "            target_feats=[target_feats_h, target_feats_m, target_feats_l], \n",
    "            voxel_size=config.first_subsampling_dl,\n",
    "            use_cuda=use_cuda)\n",
    "        source_npy, target_npy, source_feats_npy, target_feats_npy = after_vote\n",
    "\n",
    "        source, target = npy2pcd(source_npy), npy2pcd(target_npy)\n",
    "        \n",
    "        source_feats, target_feats = npy2feat(source_feats_h), npy2feat(target_feats_h)\n",
    "        pred_T, estimate, result = execute_global_registration(\n",
    "            source=source,\n",
    "            target=target,\n",
    "            source_feats=source_feats,\n",
    "            target_feats=target_feats,\n",
    "            voxel_size=dist_thresh_maps['10000']\n",
    "        )\n",
    "        \n",
    "        transformations.append(pred_T)\n",
    "    \n",
    "    corrs_pred = np.unique(np.asarray(result.correspondence_set).T[0])\n",
    "    len_coors_l.append(len(corrs_pred))\n",
    "    raw_coords_src = torch.tensor(denorm(pcd2npy(estimate), metadata)).to(coords_src.device)\n",
    "    raw_coords_tgt = torch.tensor(denorm(pcd2npy(target), metadata)).to(coords_tgt.device)\n",
    "\n",
    "    coords_src_raw = coords_src_raw.cpu().detach().numpy()\n",
    "    coords_tgt_raw = coords_tgt_raw.cpu().detach().numpy()\n",
    "    coords_tgt_full = coords_tgt_full.cpu().detach().numpy()\n",
    "    coords_transformed_src = raw_coords_src.cpu().detach().numpy()\n",
    "\n",
    "    inds = inputs['inds'][0].cpu().detach().numpy()\n",
    "    faces = inputs['faces'][0].cpu().detach().numpy()\n",
    "    \n",
    "    model_nonrigid.load_pcds(raw_coords_src.float(), raw_coords_tgt.float(), inds=corrs_pred, search_radius=0.0375)\n",
    "    warped_pcd, hist, iter_cnt, timer = model_nonrigid.register(visualize=args.vis, timer = timer)\n",
    "    warped_pcd = warped_pcd.cpu().numpy()\n",
    "    \n",
    "    \n",
    "    t2 = time.time()\n",
    "    wall_time_models.append(t2-t1)\n",
    "    \n",
    "    displacement_ngenet = raw_coords_src.cpu().detach().numpy() - coords_src_raw\n",
    "    displacement_pred = warped_pcd - coords_src_raw\n",
    "    displ_ngenet.append(displacement_ngenet)\n",
    "    displ.append(displacement_pred)\n",
    "    \n",
    "    if not args.use_real:\n",
    "        registered_cd = compute_truncated_chamfer_distance(\n",
    "            torch.tensor(warped_pcd).unsqueeze(0), \n",
    "            torch.tensor(coords_tgt_full).unsqueeze(0), \n",
    "            trunc=1e+9\n",
    "        ).item()\n",
    "        \n",
    "        nonregistered_cd = compute_truncated_chamfer_distance(\n",
    "            torch.tensor(coords_src_raw).unsqueeze(0), \n",
    "            torch.tensor(coords_tgt_full).unsqueeze(0), \n",
    "            trunc=1e+9\n",
    "        ).item()\n",
    "\n",
    "        ngenet_cd = compute_truncated_chamfer_distance(\n",
    "            torch.tensor(coords_transformed_src).unsqueeze(0).float(), \n",
    "            torch.tensor(coords_tgt_full).unsqueeze(0), \n",
    "            trunc=1e+9\n",
    "        ).item()\n",
    "\n",
    "        # MEAN DISPLACEMENT ERROR\n",
    "        mde = mean_displacement_error(\n",
    "            displacement_pred, \n",
    "            displacement_gt\n",
    "        )\n",
    "        mdes.append(mde)\n",
    "\n",
    "        # UNCERTAINTY PREDICTION\n",
    "        # based on source target distance\n",
    "        dist = torch.norm(torch.tensor(warped_pcd).unsqueeze(1) - torch.tensor(coords_tgt_raw).unsqueeze(0), dim=2)\n",
    "        minimumDistances, _ = dist.min(dim=1)\n",
    "        minimumDistances = torch.tanh(minimumDistances)\n",
    "        \n",
    "        visible_point_ratio.append(coords_tgt.shape[0]/5995)\n",
    "\n",
    "        l_inds = [v for u, v in landmarks.items()]\n",
    "        pred_landmarks = [warped_pcd[i] for i in l_inds]\n",
    "        pre_landmarks = [coords_tgt_full[i] for i in l_inds]\n",
    "        \n",
    "        lndmk = landmark_loss(pre_landmarks, pred_landmarks)\n",
    "        ind = metadata[args.dataset_split][pair_ind].split(\"/\")[1]\n",
    "        landmark_loss_l.append(lndmk)\n",
    "        ngenet_cd_l.append(ngenet_cd)\n",
    "    else:\n",
    "        \n",
    "        registered_cd = compute_truncated_chamfer_distance(\n",
    "            torch.tensor(warped_pcd).unsqueeze(0), \n",
    "            torch.tensor(coords_tgt_raw).unsqueeze(0), \n",
    "            trunc=1e+9\n",
    "        ).item()\n",
    "        \n",
    "        nonregistered_cd = compute_truncated_chamfer_distance(\n",
    "            torch.tensor(coords_src_raw).unsqueeze(0), \n",
    "            torch.tensor(coords_tgt_raw).unsqueeze(0), \n",
    "            trunc=1e+9\n",
    "        ).item()\n",
    "\n",
    "        intra_data = test_dataset_real.__getitem__(pair_ind)\n",
    "        intra_metadata = intra_data['metadata']\n",
    "        side.append(intra_metadata['patient_info']['side'])\n",
    "        status.append(intra_metadata['patient_info']['status'])\n",
    "        \n",
    "        landmarks_intra = intra_data['landmarks']\n",
    "\n",
    "        ind = pair_ind\n",
    "        if landmarks_intra != {}:\n",
    "            pred_landmarks = [warped_pcd[landmarks[k]] for k, v in landmarks_intra.items()]\n",
    "            intra_landmarks = {k:v for k, v in landmarks_intra.items()}\n",
    "            lndmk, single_loss = landmark_loss(pred_landmarks, intra_landmarks)\n",
    "            landmark_loss_l.append(lndmk)\n",
    "\n",
    "            segments = list(single_loss.keys())\n",
    "            if 'anulus' in segments:\n",
    "                anulus.append(single_loss['anulus'])\n",
    "            else:\n",
    "                anulus.append(float('nan'))\n",
    "\n",
    "            if 'Umbo' in segments:\n",
    "                umbo.append(single_loss['Umbo'])\n",
    "            else:\n",
    "                umbo.append(float('nan'))\n",
    "            \n",
    "            if 'malleus handle' in segments:\n",
    "                malleus_handle.append(single_loss['malleus handle'])\n",
    "            else:\n",
    "                malleus_handle.append(float('nan'))\n",
    "\n",
    "            if 'long process of incus' in segments:\n",
    "                long_process_of_incus.append(single_loss['long process of incus'])\n",
    "            else:\n",
    "                long_process_of_incus.append(float('nan'))\n",
    "\n",
    "            if 'stape' in segments:\n",
    "                stape.append(single_loss['stape'])\n",
    "            else:\n",
    "                stape.append(float('nan'))\n",
    "        else:\n",
    "            anulus.append(float('nan'))\n",
    "            umbo.append(float('nan'))\n",
    "            malleus_handle.append(float('nan'))\n",
    "            long_process_of_incus.append(float('nan'))\n",
    "            stape.append(float('nan'))\n",
    "            landmark_loss_l.append(-1)\n",
    "        mde = -1\n",
    "    \n",
    "    overlap = len(target_npy)/len(source_npy)\n",
    "    overlap_scores.append(overlap)\n",
    "    registered_cd_l.append(registered_cd)\n",
    "    nonregistered_cd_l.append(nonregistered_cd)\n",
    "    mean_displacement_error_l.append(mde)\n",
    "    \n",
    "    \n",
    "    output_mesh = trm.Trimesh(warped_pcd, faces)\n",
    "    _=output_mesh.export(f'test_output_folder/predictions/prediction_{ind}.stl')\n",
    "    oct_pcd = o3d.geometry.PointCloud()\n",
    "    oct_pcd.points = o3d.utility.Vector3dVector(np.array(warped_pcd))\n",
    "    o3d.io.write_point_cloud(f'test_output_folder1/predictions/prediction_{ind}.ply', oct_pcd)\n",
    "    oct_pcd = o3d.geometry.PointCloud()\n",
    "    oct_pcd.points = o3d.utility.Vector3dVector(coords_tgt_raw)\n",
    "    o3d.io.write_point_cloud(f'test_output_folder1/target shape/target_{ind}.ply', oct_pcd)\n",
    "\n",
    "    np.save(f'test_output_folder/pred_corrs/pred_corrs_{ind}.npy', np.asarray(result.correspondence_set))\n",
    "    np.save(f'test_output_folder/ndp_hist/ndp_hist_{ind}.npy', hist)\n",
    "    np.save(f'test_output_folder/other_scores/overlap_saliency_{ind}.npy', feats_src_h[:, -2].cpu().numpy())\n",
    "\n",
    "    if pair_ind == -1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_dict_exvivo = dict(\n",
    "    nonregistered_cd_l=nonregistered_cd_l,\n",
    "    registered_cd_l=registered_cd_l,\n",
    "    mean_displacement_error_l=mean_displacement_error_l,\n",
    "    overlap_scores=overlap_scores,\n",
    "    landmark_loss_l=[i if i != -1 else float('nan') for i in landmark_loss_l],\n",
    "    wall_time_models=wall_time_models,\n",
    "    ngenet_cd_l=ngenet_cd_l,\n",
    "    visible_point_ratio=visible_point_ratio,\n",
    "    displ_ngenet=displ_ngenet,\n",
    "    displ=displ,\n",
    "    len_coors_l=len_coors_l,\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(results_dict_exvivo)\n",
    "df.to_csv('finalResults/results_exvivo_ngenet_variance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-registered cd score: 7.326125677241835\n",
      "Registered cd score: 2.2037578405335894\n",
      "Overlap of pointclouds: 1.0\n",
      "Mean displacement error:  -1\n",
      "Landmark loss: 2.0358495961072816\n",
      "Wall time: 7.6358127871225046 s\n"
     ]
    }
   ],
   "source": [
    "print('Non-registered cd score:', mean(nonregistered_cd_l))\n",
    "print('Registered cd score:', mean(registered_cd_l))\n",
    "print('Overlap of pointclouds:', mean(overlap_scores))\n",
    "print('Mean displacement error: ', mean(mean_displacement_error_l))\n",
    "print('Landmark loss:', mean([i for i in landmark_loss_l if i != -1]))\n",
    "print('Wall time:', mean(wall_time_models), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] invalid color in PaintUniformColor, clipping to [0, 1]\n"
     ]
    }
   ],
   "source": [
    "colors = np.zeros((5995, 3))\n",
    "colors[:, 1] = feats_src_h[:, -2].cpu().numpy()\n",
    "\n",
    "warped_pcds = npy2pcd(raw_coords_src.cpu().numpy())\n",
    "warped_pcds.colors = o3d.utility.Vector3dVector(colors)\n",
    "target = npy2pcd(raw_coords_tgt.cpu().numpy())\n",
    "target.paint_uniform_color([255, 0, 0])\n",
    "vis_plys([warped_pcds, target], need_color=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer distance: 1.8108539581298828\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'warped_pcds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2272\\2885513226.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Chamfer distance: {registered_cd_l[sample_index]}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvis_plys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnpy2pcd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarped_pcds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnpy2pcd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_raw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'warped_pcds' is not defined"
     ]
    }
   ],
   "source": [
    "sample_index = 5\n",
    "print(f'Chamfer distance: {registered_cd_l[sample_index]}')\n",
    "vis_plys([npy2pcd(warped_pcds[sample_index]), npy2pcd(tgt_raw[sample_index].cpu().numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Non-registered cd score: 1.459614505165906\n",
    "Registered cd score: 0.5679943127659356\n",
    "Overlap of pointclouds: 0.356882301948132\n",
    "Mean displacement error:  0.7746529\n",
    "Landmark loss: 0.4213273387613913\n",
    "Wall time: 39.942818044655645 s \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DIOME no correction\n",
    "Non-registered cd score: 4.754870608795521\n",
    "Registered cd score: 1.6517222634581632\n",
    "Overlap of pointclouds: 0.5124270225187656\n",
    "Mean displacement error:  -1\n",
    "Landmark loss: 2.2849106172687916\n",
    "Wall time: 6.274375043132088 s \n",
    "\n",
    "DIOME with correction\n",
    "NNon-registered cd score: 4.179325480793798\n",
    "Registered cd score: 2.3342912876328756\n",
    "Overlap of pointclouds: 0.3749791492910759\n",
    "Mean displacement error:  -1\n",
    "Landmark loss: 1.817811517964032\n",
    "Wall time: 8.124897069709245 s\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_dict_invivo = dict(\n",
    "    nonregistered_cd_l=nonregistered_cd_l,\n",
    "    registered_cd_l=registered_cd_l,\n",
    "    overlap_scores=overlap_scores,\n",
    "    landmark_loss_l=[i if i != -1 else float('nan') for i in landmark_loss_l],\n",
    "    wall_time_models=wall_time_models,\n",
    "    displ_ngenet=displ_ngenet,\n",
    "    displ=displ,\n",
    "    side=side,\n",
    "    status=status,\n",
    "    anulus=anulus,\n",
    "    umbo=umbo,\n",
    "    malleus_handle=malleus_handle,\n",
    "    long_process_of_incus=long_process_of_incus,\n",
    "    stape=stape\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(results_dict_invivo)\n",
    "df.to_csv('finalResults/results_invivo_ngenet2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa0ffd8d7159212f64d405a3fb5222747c57c363723456464c34b8513587ee0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
